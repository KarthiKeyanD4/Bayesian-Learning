---
title: "Lab 3 - Bayesian Learning"
author: |
  | Sreenand Sasikumar - sresa472
  | Karthikeyan Devarajan  - karde799
date: "5/8/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(mvtnorm)
x <- read.delim("rainfall.dat",header = FALSE)
x <- as.matrix(x[,1])
ebay <- read.delim("eBayNumberOfBidderData.dat",sep = "")
ebay$Const <- c()
```
  
# Questions 1  
*Normal model, mixture of normal model with semi-conjugate prior*  
The data rainfall.dat consist of daily records, from the beginning of 1948 to the end of 1983, of precipitation (rain or snow in units of $\frac{1}{100}$ inch, and records of zero precipitation are excluded) at Snoqualmie Falls, Washington. Analyze the data using the following two models.   

*a) Normal model*  
Assume the daily precipitation ${y_1,...,y_n}$ are independent normally distributed, $(y_1,..., y_n|\mu,\sigma^2 \sim N(\mu, \sigma^2)$ where both $\mu$ and $\sigma^2$ are unknown.Let $\mu \sim N(\mu_o,\tau^2_o)$ independently of $\sigma^2 \sim Inv-\chi^2(\nu_o,\sigma_o^2)$.  
*i) Implement (code!) a Gibbs sampler that simulates from the joint posterior $p(\mu,\sigma^2|y_1,...,y_n)$.The full conditional posteriors are given on the slides from Lecture 7.*  

*Likelihood*    
The likelihood for the normal distribution can be assumbed to be 
$L(\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}^n}exp\bigg(\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\bigg)$  
*Full conditional Posterior*  
$$\mu|\sigma^2,x \sim N(\mu_n,\tau_n^2)$$  
$$\sigma^2|\mu,x \sim Inv-\chi^2\bigg(\nu_n,\frac{\nu_o\sigma_o^2 + \sum_{i=1}^n(x_i - \mu)^2}{n+\nu_o}\bigg)$$    
  
```{r,echo=FALSE}
set.seed(12345)
n    <- nrow(x)
mu.x <- colMeans(x)
sigma.x   <- as.numeric(var(x))

mu     <- rep(NA, 1000)
tau    <- rep(NA, 1000)
sigma    <- rep(NA, 1000)
Ta      <- 10    # burnin
tau[1] <- 1  # initialisation
for(i in 2:1000) {   
  mu[i]  <- rnorm(n = 1, mean = mu.x, sd = sqrt(1 / (n * tau[i - 1])))    
  tau[i] <- rgamma(n = 1, shape = n / 2, scale = 2 / ((n - 1) * sigma.x + n * (mu[i] - mu.x)^2))
  samples_chisq <- rchisq(1,n)
  sigma[i] <- tau[i] * n / samples_chisq
}
mu  <- mu[-(1:Ta)]
sigma <- sigma[-(1:Ta)] 
hist(mu)
hist(sigma)
```
*ii) Analyze the daily precipitation using your Gibbs sampler in (a)-i. Evaluate the convergence of the Gibbs sampler by suitable graphical methods, for example by plotting the trajectories of the sampled Markov chains.*  
```{r,echo=FALSE,warning=FALSE}
  no_iter <- 20000
  keep.mu <- rep(0,no_iter)
  keep.sigma <- rep(0,no_iter)
  tau <- rep(0,no_iter)
  samples_chisq <- rep(0,no_iter)
  # Initial values
  keep.mu[1] <- mu.x
  keep.sigma[1] <- sigma.x
  
  for(iter in 2:no_iter){

    # sample mu|s2,Y
     tau[iter-1] <- rgamma(n = iter, shape = n / 2, scale = 2 / ((n - 1) * sigma.x + n * (mu[iter] -  mu.x)^2))
     mu[iter]  <- rnorm(n = iter, mean = mu.x, sd = sqrt(1 / (n * tau[iter - 1]))) 
     

    # sample s2|mu,Y

     samples_chisq[iter] <- rchisq(1,n)
     sigma[iter] <- tau[iter-1] * n / samples_chisq[iter-1]

    # keep track of the results
     keep.mu[iter] <- mu[iter-1]
     keep.sigma[iter] <- sigma[iter-1]*1000

    # Plot the samples every 10000 iterations
     if(iter%%10000==0){
       par(mfrow=c(1,2))
       plot(keep.mu[1:iter],type="l",ylab="mu",xlim = c(0,1000))
       plot(keep.sigma[2:iter],type="l",ylab="sigma",ylim = c(0.6,0.7),xlim=c(0,1000))
     }
  }

```
  
*b) Mixture normal Model*  
Let us now instead assume that the daily precipitation $y_1, ..., y_n$ follow an two-component mixture of normals model:  
$p(y_i|\mu,\sigma^2,\pi) = \pi.N(y_i|\mu_1,\sigma_1^2) + (1-\pi)N(y_i|\mu_2,\sigma_2^2)$
where,  
$\mu = (\mu_1,\mu_2)$ and $\sigma^2 = (\sigma_1^2,\sigma_2^2)$  

*Use the Gibbs sampling data augmentation algorithm in NormalMixtureGibbs.R(available under Lecture 7 on the course page) to analyze the daily precipitation data. Set the prior hyperparameters suitably. Evaluate the convergence of the sampler.*  
```{r,echo=FALSE}
nComp <- 4    # Number of mixture components
# Prior options
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(0,nComp) # Prior mean of mu
tau2Prior <- rep(10,nComp) # Prior std of mu
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(4,nComp) # degrees of freedom for prior on sigma2

# MCMC options
nIter <- 100 # Number of Gibbs sampling draws

# Plotting options
plotFit <- TRUE
lineColors <- c("blue", "green", "magenta", 'yellow')
sleepTime <- 0.1 # Adding sleep time between iterations for plotting
################   END USER INPUT ###############

###### Defining a function that simulates from the 
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
  nCat <- length(param)
  piDraws <- matrix(NA,nCat,1)
  for (j in 1:nCat){
    piDraws[j] <- rgamma(1,param[j],1)
  }
  piDraws = piDraws/sum(piDraws) # Diving every column of piDraws by the sum of the elements in that column.
  return(piDraws)
}

# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
  n <- dim(S)[1]
  alloc <- rep(0,n)
  for (i in 1:n){
    alloc[i] <- which(S[i,] == 1)
  }
  return(alloc)
}

# Initial value for the MCMC
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
mu <- quantile(x, probs = seq(0,1,length = nComp))
sigma2 <- rep(var(x),nComp)
probObsInComp <- rep(NA, nComp)

# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x[,1])$density))


for (k in 1:nIter){
  message(paste('Iteration number:',k))
  alloc <- S2alloc(S) # Just a function that converts between different representations of the group allocations
  nAlloc <- colSums(S)
  print(nAlloc)
  # Update components probabilities
  pi <- rDirichlet(alpha + nAlloc)
  print(pi)
  # Update mu's
  for (j in 1:nComp){
    precPrior <- 1/tau2Prior[j]
    precData <- nAlloc[j]/sigma2[j]
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(x[alloc == j])
    tau2Post <- 1/precPost
    mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    print(mu)
  }
  
  # Update sigma2's
  for (j in 1:nComp){
    sigma2[j] <- rScaledInvChi2(1, df = nu0[j] + nAlloc[j], scale = (nu0[j]*sigma2_0[j] + sum((x[alloc == j] - mu[j])^2))/(nu0[j] + nAlloc[j]))
  }
  
  # Update allocation
  for (i in 1:nObs){
    for (j in 1:nComp){
      probObsInComp[j] <- pi[j]*dnorm(x[i], mean = mu[j], sd = sqrt(sigma2[j]))
    }
    S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
  }
  
  # Printing the fitted density against data histogram
  # if (plotFit && (k%%1 ==0)){
  #   effIterCount <- effIterCount + 1
  #   hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = paste("Iteration number",k), ylim = ylim)
  #   mixDens <- rep(0,length(xGrid))
  #   components <- c()
  #   for (j in 1:nComp){
  #     compDens <- dnorm(xGrid,mu[j],sd = sqrt(sigma2[j]))
  #     mixDens <- mixDens + pi[j]*compDens
  #     lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
  #     components[j] <- paste("Component ",j)
  #   }
  #   mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
  #   
  #   lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
  #   legend("topleft", box.lty = 1, legend = c("Data histogram",components, 'Mixture'), 
  #          col = c("black",lineColors[1:nComp], 'red'), lwd = 2)
  #   Sys.sleep(sleepTime)
  # }
  
}

hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density")
lines(xGrid, mixDensMean, type = "l", lwd = 2, lty = 4, col = "red")
lines(xGrid, dnorm(xGrid, mean = mean(x), sd = apply(x,2,sd)), type = "l", lwd = 2, col = "blue")
legend("topright", box.lty = 1, legend = c("Data histogram","Mixture density","Normal density"), col=c("black","red","blue"), lwd = 2)

```
  
*c) Graphical Comparison*  
Plot the following densities in one figure: 1) a histogram or kernel density estimate of the data. 2) Normal density $N(y_i|\mu,\sigma^2)$ in (a) 3) Mixture of normals density $N(y_i|\mu,\sigma^2,\pi)$ in (b). Base your plots on the mean over all posterior draws.  
```{r,echo=FALSE}
hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density",ylim = c(0.000,0.040))
lines(xGrid, mixDensMean, type = "l", lwd = 2, lty = 4, col = "red")
lines(seq(-50,500,0.01),dnorm(seq(-50,500,0.01),mean(keep.mu,na.rm = TRUE),sqrt(mean(keep.sigma[-c(3)],na.rm = TRUE))),type = "l",col = "blue",lwd = 2)
legend("topright", box.lty = 1, legend = c("Mixture density","Normal density"), col=c("red","blue"), lwd = 2)
```
  

# 2.Metropolis Random Walk for Poisson regression.  
a) Consider the following Poisson regression model    
$$y_i|\beta \sim Poisson[exp(x_i^T\beta)], i = 1,...,n$$    

where $y_i$ is the count for the ith observation in the sample and $x_i$ is the p-dimensional
vector with covariate observations for the ith observation. Use the data set eBayNumberOfBidderData.dat. This dataset contains observations from 1000 eBay auctions of coins. The response variable is nBids and records the number of bids in each auction.  
The joint posterior of parameters $\beta,\lambda,\sigma^2$for the poisson regression can be defined as    
$\beta|\sigma^2,\lambda,X \sim N(\mu_n,\Omega_n^{-1})$. Therefore gibbs sampler for beta is normally distributed.   
$\sigma^2|\lambda,X \sim Inv-\chi^2(\nu_n,\sigma_n^2)$. The gibbs sampler for the $\sigma^2$ is inverse chi - square.   
$p(\lambda|X) \propto \sqrt{\frac{|\Omega_o|}{|X`X + \Omega_o|}}\bigg(\frac{\nu_n\sigma_n^2}{2}\bigg)^{-v_n/2}.p(\lambda)$  The gibbs sampler for $\lambda$ can be defined in the above formula.   

```{r,echo=FALSE}
set.seed(12345)
model_glm <- glm(nBids ~.,family = "poisson",data = ebay)
coeff_signifi <- names(model_glm$coefficients)[model_glm$coefficients < 0.05]
cat("The significant coefficients are",coeff_signifi,"\n")
```
b) Beta using optim function    
Usually, the coefficients determines the correlation between the covariates and the output factor. According to the coefficients, all factors are significants.  
The $\beta$ prior is given as $\beta \sim N[0,100(X^T.X)^{-1}]$ where X is n * p covariate matrix.  
Assume the first posterior density is approximately multivariate normal.  
$\beta|y \sim N(\beta,J_y^{-1}(\beta))$     

```{r,echo=FALSE,warning=FALSE}
X <- as.matrix(ebay[,-1])
Y <- as.matrix(ebay[,1])
V <- 100*solve(t(X)%*%X)
covNames <- names(X) 
nPara <- dim(X)[2] 
Poison <- function(betaVect,y,X,Sigma){
  nPara <- length(betaVect);
  linPred <- X%*%betaVect;
  logLik <- sum( linPred*y - exp(linPred))
  if (abs(logLik) == Inf) logLik = -20000
  logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE)
  return(logLik + logPrior)
}
# initial value
initVal <- as.vector(rep(0,dim(X)[2])) 
# use optim to minimize
OptimResults<-optim(initVal,Poison,gr=NULL,Y,X,V,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Printing the results to the screen
postMode <- OptimResults$par
#  Posterior covariance matrix is -inv(Hessian)
postCov <- -solve(OptimResults$hessian) 
# Naming the coefficient by covariates
names(postMode) <- covNames
# Computing approximate standard deviations.
approxPostStd <- sqrt(diag(postCov)) 
# Naming the coefficient by covariates
names(approxPostStd) <- covNames 

```
  
c) Now, let's simulate from the actual posterior of $\beta$ using the Metropolis algorithm and compare with the approximate results in b). Program a general
function that uses the Metropolis algorithm to generate random draws from an arbitrary posterior density. In order to show that it is a general function for
any model, I will denote the vector of model parameters by $\theta$. Let the proposal density be the multivariate normal density mentioned in Lecture 8 (random
walk Metropolis):  

$$\theta_p|\theta^{(i-1)}\sim N(\theta^{(i-1)},c~\Sigma)$$

where $\Sigma= J_y^-1 ( \tilde \beta)$ obtained in b). The value c is a tuning parameter and should be an input to your Metropolis function. The user of your Metropolis function should be able to supply her own posterior density function, not necessarily for the Poisson regression, and still be able to use your Metropolis function. This is not so straightforward, unless you have come across function objects in R and the triple dot (...) wildcard argument. I have posted a note (HowToCodeRWM.pdf) on the course web page that describes how to do this in R. Now, use your new Metropolis function to sample from the posterior of $\beta$ in the Poisson regression for the eBay dataset. Assess MCMC convergence by graphical methods.  
```{r,echo=FALSE}
Metropolis_Poison <- function(N, c, sigma, Func, theta, ...){
  samples <- matrix(theta,nrow=N,ncol=length(theta))
  lpd <- c()
  for(i in 2:N){
    curr <- as.vector(mvrnorm(n=1, samples[i-1,], c*sigma))
    current <- Func(curr, ...)
    target <- Func(samples[i-1,], ...)
    alpha <- min(1,exp(current-target))
    U <- runif(1,min=0,max=1)
    if(U<alpha){
      samples[i,] <- curr
    }else{
      samples[i,] <- samples[i-1,]
    }
    lpd[i] <- lpd
  }
  return(list("Sample"=samples,"lpd"=lpd))
}
Metro_sample <- Metropolis_Poison(1000, c=0.1, sigma = V, Func=Poison, theta = initVal, Y, X, V)
par(mfrow=c(3,3))
for(i in 1:8){
  plot(Metro_sample$Sample[,i],type="l",
       main=paste('Covergence of ',colnames(X)[i]),
       xlab='index',ylab='value',lwd=2)
}
```
  
d)  Use the MCMC draws from c) to simulate from the predictive distribution of the number of bidders in a new auction with the characteristics below. Plot the predictive distribution. What is the probability of no bidders in this new auction?  

```{r,echo=FALSE}
# bid <- c(1,1,1,0,0,0,1,0.5)
# for(i in 1:8){
#   betas <- rmvnorm(100,mean=Metro_sample$Sample*bid)
# }
```
# Appendix
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
  

# Reference  
1.https://www4.stat.ncsu.edu/~reich/ABA/code/NN2  
2.http://www2.stat.duke.edu/~rcs46/modern_bayes17/lecturesModernBayes17/lecture-6/06-gibbs.pdf  
3.https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf  
4. Bayesian Learning lecture 7 and GibbsBivaraite.R from the lecture.
5.https://www.ime.unicamp.br/~cnaber/optim%202.pdf

